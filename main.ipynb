{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd94627",
   "metadata": {},
   "source": [
    "#### \n",
    "Pradhyumn Singh\n",
    "GTID: 903630476"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd7726",
   "metadata": {},
   "source": [
    "### Libraries Used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042b5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic data loading/storing/manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas_datareader as pdr\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Plotting Library\n",
    "import seaborn as sns\n",
    "\n",
    "#Libraries for web scrapping S&P data\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "#Libraries for feature creation & testing\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from ta.momentum import RSIIndicator,kama,PercentagePriceOscillator\n",
    "\n",
    "\n",
    "#Libraries for implementing the models used \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4ded1",
   "metadata": {},
   "source": [
    "### Implemented Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c4b19",
   "metadata": {},
   "source": [
    "#### Function for fetching the S&P ticker lists (used as large Universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d51a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text[:-1]\n",
    "        tickers.append(ticker)\n",
    "    \n",
    "    df_tickers = pd.DataFrame(tickers)\n",
    "    df_tickers.to_csv('sp500tickers.csv', header=False, index=False)\n",
    "    with open(\"sp500tickers.pickle\", \"wb\") as f:\n",
    "        pickle.dump(tickers, f)\n",
    "        \n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346ce4f",
   "metadata": {},
   "source": [
    "#### Functions for Stock Data Loading & Data Wrangling (Filling, Scaling, Binary Return Conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4392f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_data(ticker):    \n",
    "    stock_data=pdr.DataReader(ticker,'yahoo',start_date,end_date)['Adj Close'].to_frame().rename(columns={'Adj Close':''}).add_suffix(ticker)\n",
    "    \n",
    "    #filling missing values\n",
    "    stock_data.ffill(axis=0,inplace=True)\n",
    "    stock_data.bfill(axis=0,inplace=True)  \n",
    "    \n",
    "    return stock_data\n",
    " \n",
    "def scale_data(dataframe):    \n",
    "    #using Robust Scaler to scale the data (efficient in case of existence of outliers)\n",
    "    scaler=RobustScaler()\n",
    "    scaled_data = scaler.fit_transform(dataframe.to_numpy())\n",
    "    scaled_data = pd.DataFrame(scaled_data, columns=dataframe.columns,index=dataframe.index)\n",
    "    \n",
    "    return scaled_data\n",
    "\n",
    "def calculate_binary_returns(ticker_data):\n",
    "    return_data=ticker_data.pct_change().add_suffix('_Returns')\n",
    "    #convert returns into binary values (1 as profit and -1 as zero/negative profit)\n",
    "    return_data.iloc[:,0]=np.where(return_data.iloc[:,0]>0,1,-1)\n",
    "    \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108aceb",
   "metadata": {},
   "source": [
    "#### Functions for creating the dataset for explanatory variables (Both fundamental & Technical Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8a3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_macro_feature_data():\n",
    "    #Change in Vix\n",
    "    Vix_data=pdr.DataReader('^VIX','yahoo',start_date,end_date)['Adj Close'].to_frame().rename(columns={'Adj Close':''}).add_suffix('Vix')\n",
    "    Vix_data=Vix_data.diff().add_suffix('_%Change')\n",
    "\n",
    "    #S&P Return\n",
    "    SPX_data=pdr.DataReader('^GSPC','yahoo',start_date,end_date)['Adj Close'].to_frame().rename(columns={'Adj Close':''}).add_suffix('S&P')\n",
    "    SPX_data=SPX_data.pct_change().add_suffix('_%Change')\n",
    "\n",
    "    #S&P GSCI Return\n",
    "    SPGSCI_data=pdr.DataReader('^SPGSCI','yahoo',start_date,end_date)['Adj Close'].to_frame().rename(columns={'Adj Close':''}).add_suffix('S&PGSCI')\n",
    "    SPGSCI_data=SPGSCI_data.pct_change().add_suffix('_%Change')\n",
    "\n",
    "    #WTI Oil futures Return\n",
    "    WTI_data=pdr.DataReader('CL=F','yahoo',start_date,end_date)['Adj Close'].to_frame().rename(columns={'Adj Close':''}).add_suffix('WTI')\n",
    "    WTI_data=WTI_data.pct_change().add_suffix('_%Change')\n",
    "\n",
    "    #Change in Fed Fund rate\n",
    "    Fed_rate_data=pdr.DataReader(\"DFF\",'fred',start_date,end_date)\n",
    "    Fed_rate_data=Fed_rate_data.diff(1).add_suffix('_Change')\n",
    "\n",
    "    #Moody's Seasoned AAA Corporate Bond Yield Relative to Yield on 10-Year Treasury Constant Maturity\n",
    "    AAA_treasury_spread=pdr.DataReader('AAA10Y','fred',start_date,end_date).rename(columns={'AAA10Y':'AAA_treasury_spread'})\n",
    "    AAA_treasury_spread=AAA_treasury_spread.diff(1).add_suffix('_Change')\n",
    "\n",
    "    #10-Year Treasury Constant Maturity Minus 3-Month Treasury Constant Maturity \n",
    "    Term_spread=pdr.DataReader('T10Y3M','fred',start_date,end_date).rename(columns={'T10Y3M':'Term_spread'})\n",
    "    Term_spread=Term_spread.diff(1).add_suffix('_Change')\n",
    "\n",
    "    feature_data=pd.concat([Vix_data,SPX_data,SPGSCI_data,WTI_data,Fed_rate_data,AAA_treasury_spread,Term_spread],axis=1,join='inner').shift(1)\n",
    "    feature_data.dropna(inplace=True)\n",
    "    \n",
    "    feature_data_scaled=scale_data(feature_data)\n",
    "        \n",
    "    return(feature_data_scaled)\n",
    "\n",
    "\n",
    "def technical_indicators(ticker_data):\n",
    "    # Percentage Price Oscialltor\n",
    "    ticker_data['MA_12'] = ticker_data.iloc[:,0].rolling(5).mean()\n",
    "    ticker_data['MA_26'] = ticker_data.iloc[:,0].rolling(26).mean()\n",
    "    ticker_data['PPO']= ((ticker_data['MA_12'] - ticker_data['MA_26']) / ticker_data['MA_26'])\n",
    "    \n",
    "    # Relative strength indicator\n",
    "    ticker_data['RSI'] = RSIIndicator(ticker_data.iloc[:,0]).rsi()\n",
    "\n",
    "    #ticker_data['Last_Closed'] = ticker_data.iloc[:,0]\n",
    "    ticker_data['Return_Average']=ticker_data.iloc[:,0].pct_change().rolling(5).mean()\n",
    "    \n",
    "    ticker_data_scaled=scale_data(ticker_data)\n",
    "    \n",
    "    return ticker_data_scaled[['PPO','Return_Average','RSI']]\n",
    "\n",
    "\n",
    "def check_multicollinearity(feature_data):  \n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = feature_data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(feature_data.values, i) for i in range(len(feature_data.columns))]\n",
    "    \n",
    "    vif_data.set_index('feature',inplace=True)\n",
    "    print(vif_data)\n",
    "\n",
    "    sns.set(rc = {'figure.figsize':(10,8)})\n",
    "    sns.heatmap(feature_data.corr(), cbar=False, cmap=\"BuGn\", linewidths=3, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c2b0d",
   "metadata": {},
   "source": [
    "#### Functions for Model Predictions & Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82d3910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN (X_train, X_test, y_train, y_test,params,tuning):\n",
    "    \n",
    "    if tuning==1:\n",
    "        knn_model = KNeighborsClassifier(**params)\n",
    "    else:\n",
    "        knn_model = KNeighborsClassifier()\n",
    "\n",
    "    \n",
    "    knn_model.fit(X_train, y_train)\n",
    "    \n",
    "    # predictions\n",
    "    y_predicted=knn_model.predict(X_test)\n",
    "        \n",
    "    #Accuracy Metrics: \n",
    "    precision=metrics.precision_score(y_test, y_predicted)\n",
    "    recall=metrics.recall_score(y_test, y_predicted)\n",
    "    F1_score=metrics.f1_score(y_test, y_predicted)\n",
    "    accuracy=metrics.accuracy_score(y_test, y_predicted)\n",
    "        \n",
    "    y_pred_prob = knn_model.predict_proba(X_test)[::,1]\n",
    "    AUC_score=metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "    \n",
    "    return accuracy, precision, recall, F1_score, AUC_score\n",
    "\n",
    "\n",
    "def LR (X_train, X_test, y_train, y_test,params,tuning):\n",
    "    if tuning==1:\n",
    "        lr_model = LogisticRegression(**params)\n",
    "    else:\n",
    "        lr_model = LogisticRegression()\n",
    "    \n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # predictions\n",
    "    y_predicted=lr_model.predict(X_test)\n",
    "    \n",
    "    #Accuracy Metrics: \n",
    "    precision=metrics.precision_score(y_test, y_predicted)\n",
    "    recall=metrics.recall_score(y_test, y_predicted)\n",
    "    F1_score=metrics.f1_score(y_test, y_predicted)\n",
    "    accuracy=metrics.accuracy_score(y_test, y_predicted)\n",
    "    \n",
    "    y_pred_prob = lr_model.predict_proba(X_test)[::,1]\n",
    "    AUC_score=metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "    \n",
    "    return accuracy, precision, recall, F1_score, AUC_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a1d87",
   "metadata": {},
   "source": [
    "#### Functions for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c7ecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For KNN:\n",
    "\n",
    "def knn_hyper_parameter_tuning(X_train,y_train):\n",
    "    param_grid = {\n",
    "    'n_neighbors': [5,7,9,11,13,15] }\n",
    "\n",
    "    knn_model = KNeighborsClassifier()\n",
    "        \n",
    "    # Grid search of parameters, using 3 fold cross validation, \n",
    "    grid_search = GridSearchCV(estimator = knn_model, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 2)\n",
    "\n",
    "    # Fitting the grid search model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "\n",
    "#For Logistic Regression:\n",
    "\n",
    "def lr_hyper_parameter_tuning(X_train,y_train):\n",
    "    param_grid = {\n",
    "    'penalty': ['elasticnet'],\n",
    "    'solver': ['saga'],\n",
    "    'l1_ratio': np.linspace(0,1,11),\n",
    "    'max_iter':[500]\n",
    "    }\n",
    "    \n",
    "        \n",
    "    lr_model = LogisticRegression()\n",
    "            \n",
    "    # Grid search of parameters, using 3 fold cross validation, \n",
    "    grid_search = GridSearchCV(estimator = lr_model, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "    # Fitting the grid search model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086505f",
   "metadata": {},
   "source": [
    "#### Main Function to generate the output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a07c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output (ticker_list,feature_data_scaled,tuning):\n",
    "    output_frame=pd.DataFrame(columns=['Ticker','Accuracy_KNN_Model','Precision__KNN_Model','Recall_KNN_Model','F1_Score_KNN_Model','AUC_KNN_Model','Accuracy_LR_Model','Precision_LR_Model','Recall_LR_Model','F1_Score_LR_Model','AUC_LR_Model'])\n",
    "\n",
    "    for ticker in ticker_list:\n",
    "        \n",
    "        #Error handling for those tickers in the large sample for which information is not available\n",
    "        try: \n",
    "            stock_data=load_stock_data(ticker)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        return_data=calculate_binary_returns(stock_data)\n",
    "        technical_data=technical_indicators(stock_data).shift(1)\n",
    "    \n",
    "        #combined_data=pd.concat([return_data,feature_data_scaled,technical_data],join='inner',axis=1)\n",
    "        combined_data=pd.concat([return_data,feature_data_scaled,technical_data],join='inner',axis=1)\n",
    "        combined_data.dropna(inplace=True)\n",
    "        \n",
    "        \n",
    "        #Only running the model for the stocks for which data of atleast two months is available \n",
    "        if combined_data.shape[0]<60:\n",
    "            continue\n",
    "        \n",
    "        #Splitting the dataset into explanatory and explained variable\n",
    "        X=combined_data.iloc[:,1:]\n",
    "        y=combined_data.iloc[:,0]\n",
    "\n",
    "        # Splitting dataset into training set and test set\n",
    "        X_train= X.iloc[0:int(X.shape[0]*60/100),:]\n",
    "        y_train= y.iloc[0:int(y.shape[0]*60/100)]\n",
    "        X_test= X.iloc[-(X.shape[0]-X_train.shape[0]):,:]\n",
    "        y_test= y.iloc[-(y.shape[0]-y_train.shape[0]):,]\n",
    "\n",
    "        if tuning==1:            \n",
    "            #finding best parameters for Random Forest\n",
    "            best_params_knn=knn_hyper_parameter_tuning(X_train, y_train)\n",
    "\n",
    "            #finding best parameters for Logistic Regression\n",
    "            best_params_en=lr_hyper_parameter_tuning(X_train, y_train)\n",
    "\n",
    "            #calculating the evaluation metrics for KNN\n",
    "            accuracy_m1, precision_m1, recall_m1, F1_score_m1, AUC_score_m1 = KNN(X_train, X_test, y_train, y_test,best_params_knn,tuning)\n",
    "            \n",
    "            #calculating the evaluation metrics for Logistic Regression\n",
    "            accuracy_m2, precision_m2, recall_m2, F1_score_m2, AUC_score_m2 = LR(X_train, X_test, y_train, y_test,best_params_en,tuning)\n",
    "\n",
    "\n",
    "        else:\n",
    "            #calculating the evaluation metrics for KNN\n",
    "            accuracy_m1, precision_m1, recall_m1, F1_score_m1, AUC_score_m1 = KNN(X_train, X_test, y_train, y_test,0,tuning)\n",
    "            \n",
    "            #calculating the evaluation metrics for Logistic Regression\n",
    "            accuracy_m2, precision_m2, recall_m2, F1_score_m2, AUC_score_m2 = LR(X_train, X_test, y_train, y_test,0,tuning)\n",
    "\n",
    "        output_frame.loc[len(output_frame.index)] = [ticker,accuracy_m1,precision_m1,recall_m1,F1_score_m1,AUC_score_m1,accuracy_m2,precision_m2,recall_m2,F1_score_m2,AUC_score_m2]\n",
    "      \n",
    "    return output_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437a2d2",
   "metadata": {},
   "source": [
    "### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63ab9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem parameters\n",
    "start_date=\"2000-01-01\"\n",
    "end_date=\"2021-11-12\"\n",
    "\n",
    "#Creating the Macro Features and checking for multicollinearity\n",
    "feature_data_scaled=create_macro_feature_data()\n",
    "#check_multicollinearity(feature_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922ad28",
   "metadata": {},
   "source": [
    "#### Output for small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a53fd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n"
     ]
    }
   ],
   "source": [
    "#reading the tickers from the csv file\n",
    "ticker_data_10=pd.read_csv(\"tickers.csv\")\n",
    "ticker_list_10=ticker_data_10.iloc[:,0].to_list()\n",
    "hyperparameter_tuning_on=1\n",
    "\n",
    "output_small_universe=generate_output(ticker_list_10,feature_data_scaled,hyperparameter_tuning_on)\n",
    "\n",
    "#exporting the data to a csv file\n",
    "output_small_universe.to_csv('Output_Small_Universe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ad80c",
   "metadata": {},
   "source": [
    "#### Output for large sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4344a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list_SnP=save_sp500_tickers()\n",
    "\n",
    "#hyperparameter_tuning kept on off for default to reduce time complexity \n",
    "hyperparameter_tuning_on=0\n",
    "output_large_universe=generate_output(ticker_list_SnP,feature_data_scaled,hyperparameter_tuning_on)\n",
    "\n",
    "#exporting the data to a csv file\n",
    "output_large_universe=output_large_universe.loc[(output_large_universe['Precision__KNN_Model']>.5141)& (output_large_universe['Precision_LR_Model']>.5141)]\n",
    "\n",
    "output_large_universe.sort_values(by='Accuracy_KNN_Model',inplace=True,ascending=False)\n",
    "output_large_universe_KNN20=output_large_universe.iloc[:20,]\n",
    "output_large_universe_KNN20.to_csv('Output_Large_Universe_KNN20.csv')\n",
    "\n",
    "output_large_universe.sort_values(by='Accuracy_LR_Model',inplace=True,ascending=False)\n",
    "output_large_universe_LR20=output_large_universe.iloc[:20,]\n",
    "output_large_universe_LR20.to_csv('Output_Large_Universe_LR20.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
